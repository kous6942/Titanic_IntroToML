# program to predict whether a passenger on the titanic would survive or not
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

titanic = sns.load_dataset('titanic')

#printing the first 12 rows of the data
titanic.head(20)

# counting the number of rows and columns
titanic.shape

# from the output we can see that there are 891 rows and 15 columns

# getting some statistics of the dataset
titanic.describe()

# understanding the size of the dataset
titanic.size

# from the output we understand that there are 13365 elements in total

# the average survival rate is 0.38
# on an average the people chose 2nd class for their travel
# the average age of a passenger was 29.6 years of age
# average fare for ticket: 32 pounds where the min was 0 and max was 512 pounds

# finding out the number of people who survived
titanic['survived'].value_counts()

# creating a barplot for visualizing survival
sns.countplot(x='survived', data = titanic, hue = 'survived', palette = 'hls')

# 0 or the bar in red is bar plot of those who did not survive
# 1 or the blue bar is the bar plot for the people who did survive
# we can see that more number of people did not survive than those who did

''' visualizing the number of survivors with column names:
who
pclass
age
sibsp
parch
sex
and embarked'''



col = ['who', 'sex','pclass', 'sibsp', 'parch' ,'embarked']

nrows = 2
ncols = 3

# subplots
fig, axs= plt.subplots(nrows, ncols, figsize = (ncols* 4.2, nrows*  4.2))

for r in range(0, nrows):
  for c in range(0, ncols):
    i = r*ncols + c # indexing the number of columns

    ax = axs[r][c]

     # Use the melt function to reshape the data for the countplot
    melted_data = titanic.melt(id_vars=['survived'], value_vars=[col[i]])
    sns.countplot(x='value', hue='survived', data=melted_data, ax=ax, palette = 'hls')



    #giving graphs titles
    ax.set_title(col[i])
    ax.legend(title = 'survived', loc = 'upper right')

plt.tight_layout() # Adjust layout to prevent overlapping
plt.show()

#understanding the survival rates of each plot

titanic.groupby('who')['survived'].mean()

titanic.groupby('sex')['survived'].mean()

titanic.groupby('pclass')['survived'].mean()

titanic.groupby('sibsp')['survived'].mean()

titanic.groupby('parch')['survived'].mean()

titanic.groupby('embarked')['survived'].mean()

#observing survival rates simultaneously by category
titanic.groupby(['who', 'pclass'])['survived'].mean(

#creating pivot table
titanic.pivot_table('survived', index = 'who', columns = 'class').plot()

# it can be observed that a woman from 1st class had the best rate of survival
# and that a man of 3rd class had the least chance of survival

titanic.groupby(['embarked', 'pclass'])['survived'].mean()
''' it can be observed that passengers who booked 1st class and were from
C had the best chance of survival'''

titanic.pivot_table('survived', index = 'embarked', columns = 'class').plot()

titanic.groupby(['sex', 'embarked'])['survived'].mean()
# females from C had the best chance of survial as compared to males from Q

titanic.pivot_table('survived', index = 'sex', columns = 'embarked').plot()

#plotting categorically based on survival
#who
sns.barplot(x='who', y='survived', data=titanic, palette = 'hls')

#sex
sns.barplot(x='sex', y='survived', data=titanic, palette = 'hls')

#class
sns.barplot(x='class', y='survived', data=titanic, palette = 'hls')

#sibling/spouse
sns.barplot(x='sibsp', y='survived', data=titanic, palette = 'hls')

#parent/child
sns.barplot(x='parch', y='survived', data=titanic, palette = 'hls')

#embarked
sns.barplot(x='embarked', y='survived', data=titanic, palette = 'hls')

#looking at survival rate by sex, class and age

age= pd.cut(titanic['age'], [0, 18, 80])
titanic.pivot_table('survived', ['sex', age], 'pclass')

#checking the same for fare wrt sex and age
titanic.pivot_table('survived', ['sex', age], 'fare')

titanic.pivot_table('survived', index = 'fare', columns = 'sex').plot()

#counting the empty values in every column
titanic.isna().sum()

#observe the values of each column and get the sum of them individually

for valu in titanic:
  print(titanic[valu].value_counts())

#drop the columns that are redudant
titanic = titanic.dropna(subset= ['embarked', 'age'])

titanic = titanic.drop(['deck', 'embark_town', 'alive', 'who', 'alone', 'class','adult_male'], axis = 1)

#counting the cleaned number of rows and columns
titanic.shape

#observing the remaining datatypes
titanic.dtypes

#changing the datatype of the objects
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

titanic['sex'] = labelencoder.fit_transform(titanic['sex'])
titanic['embarked'] = labelencoder.fit_transform(titanic['embarked'])

print(titanic['embarked'].unique())
print(titanic['sex'].unique())

#checking data types again
titanic.dtypes

#splitting the data into independent and dependent variables (M,N)
N = titanic.iloc[:, 0].values
M = titanic.iloc[:, 1:8].values


#splitting into 80%:20% for training to testing
from sklearn.model_selection import train_test_split
M_train, M_test, N_train, N_test = train_test_split(M, N, test_size = 0.2, random_state = 0)

#creating a function with a variety of models

def models(M_train, N_train):

  #using kneighbours

  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(M_train, N_train)


  #using svc linear kernel

  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state = 0)
  svc_lin.fit(M_train, N_train)

  #using svc rbf kernel

  from sklearn.svm import SVC
  svc_rbf = SVC(kernel = 'rbf', random_state = 0)
  svc_rbf.fit(M_train, N_train)

  #using decision tree


  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(M_train, N_train)

  #using random forest

  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(M_train, N_train)


  #printing the accuracy of training for each model
  print('[1]KNN training accuracy: ', knn.score(M_train, N_train))
  print('[2]SVC linear training accuracy: ', svc_lin.score(M_train, N_train))
  print('[3]SVC rbf training accuracy: ', svc_rbf.score(M_train, N_train))
  print('[4]Decision tree training accuracy: ', tree.score(M_train, N_train))
  print('[5]Random forest training accuracy: ', forest.score(M_train, N_train))
  

  return  knn, svc_lin, svc_rbf, tree, forest

#training the models

model = models(M_train, N_train)

#using confusion matrix and accuracy for the models using test data

from sklearn.metrics import confusion_matrix

for i in range(len(model)):
  cm = confusion_matrix(N_test, model[i].predict(M_test))



  #extracting TP, TN, FP, FN
  TN, FP, FN, TP = confusion_matrix(N_test, model[i].predict(M_test)).ravel()
  testscore = (TP + TN)/(TP + TN + FP + FN)


  #tp = cm[0][0]
  #tn = cm[1][1]
  #fp = cm[0][1]
  #fn = cm[1][0]


  print(cm)
  print('Model[{}] testing accuracy = "{}"'.format(i, testscore))
  print()

#random forest has the best accuracy so far
#get importance of feature

forest = model[4]
importances = pd.DataFrame({'feature' : titanic.iloc[:, 1:8].columns, 'importance' : np.round(forest.feature_importances_, 3)})
importances = importances.sort_values('importance', ascending = False).set_index('feature')
importances

#plotting the importances as bargraph
importances.plot.bar()

#printing the prediction of the random forest classifier
predi = model[4].predict(M_test)
print(predi)

print()

#printing the actual values
print(N_test)


'''pclass        int64
sex           int64
age         float64
sibsp         int64
parch         int64
fare        float64
embarked      int64'''

my_surv = [[2, 0, 18, 0, 0, 0, 2]]
papa_surv = [[1, 0, 50, 0, 0, 0, 1]]

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
fitted = sc.fit(M_train)
my_surv_scaled = sc.transform(my_surv)
papa_surv_scaled = sc.transform(papa_surv)

#printing the prediction of my survival

predi = model[4].predict(papa_surv)

print(predi)



























































